{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat Tutorial\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by the [CS228 tutorial](https://github.com/kuleshov/cs228-material/blob/master/tutorials/python/cs228-python-tutorial.ipynb) by Volodomyr Kuleshov and Isaac Caswell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "---\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "<div style=\"float: right; padding-right: 2em; padding-top: 2em;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/helmholtz-analytics/heat/master/doc/images/logo.png\"></img>\n",
    "</div>\n",
    "\n",
    "* [Installation](#Installation)\n",
    "    * [Dependencies](#Dependencies)\n",
    "    * [Dependencies](#Dependencies)\n",
    "* [Heat Arrays](#Heat-Arrays)\n",
    "    * [Data Types](#Data-Types)\n",
    "    * [Operations](#Operations)\n",
    "    * [Indexing](#Indexing)\n",
    "* [Parallel Processing](#Parallel-Processing)\n",
    "    * [GPUs](#Dependencies)\n",
    "    * [Distributed Computing](#Distributed-Computing)\n",
    "    * [Parallel Interactive Interpreter](#Parallel-Interactive-Interpreter)\n",
    "    * [Dos and Don'ts](#Dos-and-Don'ts)\n",
    "\n",
    "Heat is a flexible and seamless open-source software for high performance data analytics and machine learning. It provides highly optimized algorithms and data structures for multi-dimensional array computations using CPUs, GPUs, and distributed cluster systems. The goal of Heat is to fill the gap between data analytics and machine learning libraries with a strong focus on single-node performance on one side, and traditional high-performance computing (HPC) on the other. Heat's dtype Python-first programming interface integrates seamlessly with the existing data science ecosystem. Heat's interface makes it as effortless as using numpy to write scalable scientific and data science applications that go beyond the computational and memory capabilities of your laptop or desktop.\n",
    "\n",
    "For this tutorial, we assume that you are somewhat proficient in the Python programming language. Equally, it is beneficial that you have worked with vectorized multi-dimensional array data structures before, such as those offered by NumPy, Matlab, or R. If you haven't, or if you would like to refresh your knowledge, you may find the following ressources useful: [CS228 Python and NumPy Tutorial](https://github.com/kuleshov/cs228-material/blob/master/tutorials/python/cs228-python-tutorial.ipynb), [NumPy for MATLAB users](https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html), and [NumPy for R users](http://mathesaurus.sourceforge.net/r-numpy.html)\n",
    "\n",
    "In line with this tutorial, we will cover the following topics:\n",
    "\n",
    "* Installation and setup of Heat\n",
    "* Working with Heat arrays, operations, indexing etc.\n",
    "* Utilizing Heat's scalable parallel processing capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "---\n",
    "\n",
    "In most use cases, the best way to install Heat on your system is to use the official pre-built package from the Python Package index (PyPi) as follows.\n",
    "\n",
    "```bash\n",
    "python -m pip install heat\n",
    "```\n",
    "\n",
    "You may need to use the `--user` flag or a [virtual environment](https://docs.python.org/3/library/venv.html) on systems where you do not have sufficient privileges.\n",
    "\n",
    "You can also install the latest and greatest Heat version by cloning the Heat source code repository and doing a manual installation.\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/helmholtz-analytics/heat && cd heat && pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "Heat requires you to have an [MPI](https://hpc-tutorials.llnl.gov/mpi/) installation on your system in order to enable parallel processing capabilities. If not already present on your system (also applies to laptops, desktops, etc.), you can obtain it through your system's package manager (here: OpenMPI), e.g.:\n",
    "\n",
    "```bash\n",
    "apt-get install libopenmpi-dev (Ubuntu, Debian)\n",
    "dnf install openmpi-devel (Fedora)\n",
    "yum install openmpi-devel (CentOS)\n",
    "```\n",
    "\n",
    "Installing these dependencies usually requires administrator privileges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Features\n",
    "\n",
    "Heat may be installed with several optional features, i.e. GPU support on top of CUDA or HDF5 and NetCDF4 (parallel) I/O. If you would like to use these features, this how you can enable them\n",
    "\n",
    "* GPU support—ensure that CUDA is installed on your system. You may find an installation guide [here](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html).\n",
    "* HDF5 support—install HDF5 via your system's package manager, preferably with parallel I/O capabilities\n",
    "\n",
    "```bash\n",
    "apt-get install libhdf5-openmpi-dev (Ubuntu, Debian)\n",
    "dnf install hdf5-openmpi-devel (Fedora)\n",
    "yum install hdf5-openmpi-devel (CentOS)\n",
    "```\n",
    "\n",
    "* NetCDF4 support—install NetCDF4 via your system's package manager, preferably with parallel I/O capabilities\n",
    "\n",
    "```bash\n",
    "apt-get install libnetcdf-dev (Ubuntu, Debian)\n",
    "dnf install netcdf-openmpi-devel (Fedora)\n",
    "yum install netcdf-openmpi-devel (CentOS)\n",
    "```\n",
    "\n",
    "When you install Heat you need to explicitly state that you also want to install all modules for HDF5 and NetCDF4 support by specifying an extras flag, i.e.:\n",
    "\n",
    "```bash\n",
    "pip install -e heat[hdf5,netcdf]\n",
    "```\n",
    "\n",
    "respectively\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/helmholtz-analytics/heat && cd heat && pip install -e .[hdf5,netcdf]\n",
    "```\n",
    "\n",
    "It is possible to exclusively install either HDF5 or NetCDF4 support by leaving out the respective extra dependency in the above command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Arrays\n",
    "---\n",
    "\n",
    "To be able to start working with Heat, we first have to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heat as ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to a NumPy array, a Heat array is a grid of values of a single (one particular) type. The number of dimensions is the number of axes of the array, while the shape of an array is a tuple of integers giving the number of elements of the array along each dimension. \n",
    "\n",
    "Heat emulates NumPy's API as closely as possible, allowing for the use of well-known array creation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([1, 2, 3], dtype=ht.int64, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([[1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.]], dtype=ht.float32, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.ones((4, 5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=ht.int32, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([[9., 9.],\n",
       "          [9., 9.],\n",
       "          [9., 9.]], dtype=ht.float32, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.full((3, 2,), fill_value=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types\n",
    "\n",
    "Heat supports various data types and operations to retrieve and manipulate the type of a Heat array. However, in contrast to NumPy, Heat is limited to logical (bool) and numerical types (uint8, int16/32/64, and float32/64). \n",
    "\n",
    "**NOTE:** by default, Heat will allocate floating-point values in single-precision, due to a much higher processing performance on GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DNDarray([[0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0.]], dtype=ht.float32, device=cpu:0, split=None),\n",
       " heat.core.types.float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ht.zeros((3, 4,))\n",
    "a, a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DNDarray([[0, 0, 0, 0],\n",
       "           [0, 0, 0, 0],\n",
       "           [0, 0, 0, 0]], dtype=ht.int64, device=cpu:0, split=None),\n",
       " heat.core.types.int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.astype(ht.int64)\n",
    "b, b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([[0, 0, 0, 0],\n",
       "          [0, 0, 0, 0],\n",
       "          [0, 0, 0, 0]], dtype=ht.int8, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.zeros((3, 4,), dtype=ht.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations\n",
    "\n",
    "Heat supports several mathematical operations, ranging from simple element-wise functions, binary arithmetic operations, and linear algebra, to more powerful reductions. Operations are by default performed on the entire array or they can be performed along one or more of its dimensions when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ht.full((3, 4,), 8)\n",
    "b = ht.ones((3, 4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([[9., 9., 9., 9.],\n",
       "          [9., 9., 9., 9.],\n",
       "          [9., 9., 9., 9.]], dtype=ht.float32, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([[7., 7., 7., 7.],\n",
       "          [7., 7., 7., 7.],\n",
       "          [7., 7., 7., 7.]], dtype=ht.float32, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.sub(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([ 0.0000,  0.8415,  0.9093,  0.1411, -0.7568], dtype=ht.float32, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.arange(5).sin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([[8., 8., 8.],\n",
       "          [8., 8., 8.],\n",
       "          [8., 8., 8.],\n",
       "          [8., 8., 8.]], dtype=ht.float32, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([4., 4., 4.], dtype=ht.float32, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Heat implements the same broadcasting rules (implicit repetion of an operation when the rank/shape of the operands do not match) as NumPy does, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=ht.int64, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.arange(10) + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DNDarray([[1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1.]], dtype=ht.float32, device=cpu:0, split=None),\n",
       " DNDarray([0, 1, 2, 3], dtype=ht.int32, device=cpu:0, split=None),\n",
       " DNDarray([[1., 2., 3., 4.],\n",
       "           [1., 2., 3., 4.],\n",
       "           [1., 2., 3., 4.]], dtype=ht.float32, device=cpu:0, split=None))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ht.ones((3, 4,))\n",
    "b = ht.arange(4)\n",
    "c = a + b\n",
    "\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "Heat allows the indexing of arrays, and thereby, the extraction of a partial view of the elements in an array. It is possible to obtain single values as well as entire chunks, i.e. slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=ht.int32, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ht.arange(10)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray(3, dtype=ht.int32, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([1, 2, 3, 4, 5, 6], dtype=ht.int32, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([0, 2, 4, 6, 8], dtype=ht.int32, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "\n",
    "Heat is extensively documented. You may find the online API reference on Read the Docs: [Heat Documentation](https://heat.readthedocs.io/). It is also possible to look up the docs in an interactive session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sum in module heat.core.arithmetics:\n",
      "\n",
      "sum(a: 'DNDarray', axis: 'Union[int, Tuple[int, ...]]' = None, out: 'DNDarray' = None, keepdim: 'bool' = None) -> 'DNDarray'\n",
      "    Sum of array elements over a given axis. An array with the same shape as ``self.__array`` except for the specified\n",
      "    axis which becomes one, e.g. ``a.shape=(1, 2, 3)`` => ``ht.ones((1, 2, 3)).sum(axis=1).shape=(1, 1, 3)``\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : DNDarray\n",
      "        Input array.\n",
      "    axis : None or int or Tuple[int,...], optional\n",
      "        Axis along which a sum is performed. The default, ``axis=None``, will sum all of the elements of the input array.\n",
      "        If ``axis`` is negative it counts from the last to the first axis. If ``axis`` is a tuple of ints, a sum is performed\n",
      "        on all of the axes specified in the tuple instead of a single axis or all the axes as before.\n",
      "    out : DNDarray, optional\n",
      "        Alternative output array in which to place the result. It must have the same shape as the expected output, but\n",
      "        the datatype of the output values will be cast if necessary.\n",
      "    keepdim : bool, optional\n",
      "        If this is set to ``True``, the axes which are reduced are left in the result as dimensions with size one. With this\n",
      "        option, the result will broadcast correctly against the input array.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> ht.sum(ht.ones(2))\n",
      "    DNDarray([2.], dtype=ht.float32, device=cpu:0, split=None)\n",
      "    >>> ht.sum(ht.ones((3,3)))\n",
      "    DNDarray([9.], dtype=ht.float32, device=cpu:0, split=None)\n",
      "    >>> ht.sum(ht.ones((3,3)).astype(ht.int))\n",
      "    DNDarray([9], dtype=ht.int64, device=cpu:0, split=None)\n",
      "    >>> ht.sum(ht.ones((3,2,1)), axis=-3)\n",
      "    DNDarray([[3.],\n",
      "             [3.]], dtype=ht.float32, device=cpu:0, split=None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ht.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Processing\n",
    "---\n",
    "\n",
    "Heat's actual power lies in the possibility to exploit the processing performance of modern accelerator hardware (GPUs) as well as distributed (high-performance) cluster systems. By itself, all operations executed on CPUs are, to a large extent, vectorized (AVX) and thread-parallelized (OpenMP). We utilize CUDA to process data on GPUs, requiring you to have a suitable nVidia device and the Message Passing Interface (MPI) for distributed computations.\n",
    "\n",
    "**NOTE:** The GPU examples below will only properly execute on a computer with a CUDA GPU. Make sure to either start the notebook on an appropriate machine or copy and paste the examples into a script and execute it on a suitable device.\n",
    "\n",
    "**NOTE: ** All examples below explaining the distributed processing capabilities need to be executed outside this notebook in a separate MPI-capable environment. We suggest to copy and paste the code snippets into a script and execute it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPUs\n",
    "\n",
    "Heat's array creation functions all support an additional parameter that which places the data on a specific device. By default, the CPU is selected, but it is also possible to directly allocate the data on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]], dtype=ht.float32, device=gpu:0, split=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.zeros((3, 4,), device='gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays on the same device can be seamlessly used in any Heat operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]], dtype=ht.float32, device=gpu:0, split=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ht.zeros((3, 4,), device='gpu')\n",
    "b = ht.ones((3, 4,), device='gpu')\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, performing operations on arrays with mismatching devices will purposefully result in an error (due to potentially large copy overhead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "a = ht.full((3, 4,), 4, device='cpu')\n",
    "b = ht.ones((3, 4,), device='gpu')\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to explicitly move an array from one device to the other and back to avoid this error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNDarray([[4., 4., 4., 4.],\n",
       "          [4., 4., 4., 4.],\n",
       "          [4., 4., 4., 4.]], dtype=ht.float32, device=cpu:0, split=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ht.full((3, 4,), 4, device='gpu')\n",
    "a.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When writing code for GPUs only, you might quickly find it tedious to explicitly place everything on the GPU by specifying the `device=` parameter. Hence, it is possible to set a default backend on which Heat will work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht.use_device('gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Computing\n",
    "\n",
    "Heat is also able to make use of distributed processing capabilities such as those in high-performance cluster systems. For this, Heat exploits the fact that the operations performed on a multi-dimensional array are usually identical for all data items. Hence, a data-parallel processing strategy can be chosen, where the total number of data items is equally divided among all processing nodes. An operation is then performed individually on the local data chunks and, if necessary, communicates partial results behind the scenes. A Heat array assumes the role of a virtual overlay of the local chunks and realizes and coordinates the computations. Please see the figure below for a visual representation of this concept.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/helmholtz-analytics/heat/master/doc/images/heat_split_array.png\" width=\"40%\"></img>\n",
    "\n",
    "The chunks are always split along a singular dimension (i.e. 1D domain decomposition) of the array. You can specify this in Heat by using the `split` paramter. This parameter is present in all relevant functions, such as array creation (`zeros(), ones(), ...`) or I/O (`load()`) functions. Examples are provided below. The result of an operation on a Heat tensor will in most cases preserve the split of the respective operands. However, in some cases the split axis might change. For example, a transpose of a Heat array will equally transpose the split axis. Furthermore, a reduction operations, e.g. `sum()` that is performed across the split axis, might remove data partitions entirely. The respective function behaviors can be found in Heat's documentation.\n",
    "\n",
    "You may also modify the data partitioning of a Heat array by using the `resplit()` function. This allows you to repartition the data as you so choose. Please note, that this should be used sparingly and for small data amounts only, as it entails significant data copying across the network. Finally, a Heat array without any split, i.e. `split=None` (default), will result in redundant copies of data on each computation node.\n",
    "\n",
    "On a technical level, Heat follows the so-called [Bulk Synchronous Parallel (BSP)](https://en.wikipedia.org/wiki/Bulk_synchronous_parallel) processing model. For the network communication, Heat utilizes the [Message Passing Interface (MPI)](https://computing.llnl.gov/tutorials/mpi/), a defacto standard on modern high-performance computing systems. It is also possible to use MPI on your laptop or desktop computer. Respective software packages are available for all major operating systems. In order to run a Heat script, you need to start it slightly differently than you are probably used to. This\n",
    "\n",
    "```bash\n",
    "python ./my_script.py\n",
    "```\n",
    "\n",
    "becomes this instead:\n",
    "\n",
    "```bash\n",
    "mpirun -p <number_of_processors> python ./my_script.py\n",
    "```\n",
    "\n",
    "Let's see some examples of working with distributed Heat\n",
    "\n",
    "**NOTE: ** In the following we will use a `(<processor_id>/<processor_count>)` prefix on each output to clearly show, what each individual process is printing. In actual application you would not observe this behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Unsplit\" data, i.e. local copies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0/2) tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)\n",
       "(1/2) tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.arange(10, split=None)  # equivalent to just saying ht.arange(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data division along the major axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0/2) tensor([0, 1, 2, 3, 4], dtype=torch.int32)\n",
       "(1/2) tensor([5, 6, 7, 8, 9], dtype=torch.int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.arange(10, split=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other split axes are also possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0/2) tensor([[1, 2],\n",
       "(0/2)         [5, 6]]),\n",
       "(1/2) tensor([[3, 4],\n",
       "(1/2)         [7, 8]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8]\n",
    "], split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartitioning of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0/2) tensor([[0., 0., 0.],\n",
       "(0/2)         [0., 0., 0.],\n",
       "(0/2)         [0., 0., 0.],\n",
       "(0/2)         [0., 0., 0.]])\n",
       "(1/2) tensor([[0., 0., 0.],\n",
       "(1/2)         [0., 0., 0.],\n",
       "(1/2)         [0., 0., 0.],\n",
       "(1/2)         [0., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ht.zeros((4, 6,), split=1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0/2) tensor([[0., 0., 0., 0., 0., 0.],\n",
       "(0/2)         [0., 0., 0., 0., 0., 0.]])\n",
       "(1/2) tensor([[0., 0., 0., 0., 0., 0.],\n",
       "(1/2)         [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.resplit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0/2) tensor([3, 4, 5, 6, 7], dtype=torch.int32)\n",
       "(1/2) tensor([8, 9, 10, 11, 12], dtype=torch.int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.arange(10, split=0) + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations between tensors with equal split or no split are fully parallelizable and therefore very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0/2) tensor([1, 2, 3, 4, 5], dtype=torch.int32)\n",
       "(1/2) tensor([6, 7, 8, 9, 10], dtype=torch.int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ht.arange(10, split=0)\n",
    "b = ht.ones((10,), split=0)\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Interactive Interpreter\n",
    "\n",
    "Heat allows you to interactively program and debug distributed code. The root process will spawn an interactive shell, that forwards the inputs to all other ranks and equally collects the output of all nodes. The interactive interpreter can be found in the Heat sources in the path `scripts/interactive.py` or can be download like this `wget https://raw.githubusercontent.com/helmholtz-analytics/heat/master/scripts/interactive.py`.\n",
    "\n",
    "You can start the interactive interpreter by invoking the following command. The `-s all` flag must be passed to the interpeter for it to work.\n",
    "\n",
    "```bash\n",
    "mpirun -s all -np <procs> python interactive.py\n",
    "```\n",
    "\n",
    "**NOTE: ** the interactive interpreter unfortunately does not support the full set of control commands, disallowing 'arrow-up' command repetition for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dos and Don'ts\n",
    "\n",
    "In this section we would like to address a few best practices for programming with Heat. While we can obviously not cover all issues, these are major pointers as how to get reasonable performance.\n",
    "\n",
    "**Dos**\n",
    "\n",
    "* Split up large data amounts\n",
    "    * often you input data set along the 'observations/samples' dimension\n",
    "    * large intermediate matrices\n",
    "* Use the Heat API\n",
    "    * computational kernels are optimized\n",
    "    * Python constructs (e.g. loops) tend to be slow\n",
    "* Potentially have a copy of certain data with different splits\n",
    "\n",
    "**Dont's**\n",
    "\n",
    "* Avoid extensive data copying, e.g.\n",
    "    * operations with operands of different splits (except None)\n",
    "    * reshape() that actually change the array dimensions (adding extra dimensions with size 1 is fine)\n",
    "* Execute everything on GPU\n",
    "    * computation-intensive operations are usually a good fit\n",
    "    * operations extensively accessing memory only (e.g. sorting) are not"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
