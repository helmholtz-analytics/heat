{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cefd34f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f608e796",
   "metadata": {},
   "source": [
    "That linear algebra is fun, is a widely accepted fact. This notebooks will guide you through some of the linear algebra fun you can realize with Heat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf0367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyparallel import Client\n",
    "rc = Client(profile=\"default\")\n",
    "rc.ids\n",
    "\n",
    "if len(rc.ids) == 0:\n",
    "    print(\"No engines found\")\n",
    "else:\n",
    "    print(f\"{len(rc.ids)} engines found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65902e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import heat as ht"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb62f7",
   "metadata": {},
   "source": [
    "## Matrix-Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fe39d1",
   "metadata": {},
   "source": [
    "The most basic operation in linear algebra is matrix-matrix multiplication (\"matmul\"). Doing it by hand for a small matrix is not difficult and in fact not very spectacular. However, in the distributed setting (e.g., on 4 GPUs) even such a simple operation is not trivial any more: just imagine you work together with 3 other people and each of you only knows one fourth of the columns of a matrix $A$ and one fourth of the rows of a matrix $B$. Together, you have to compute the product $AB$ such that in the end each of you only has one fourth of the columns of $AB$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27729f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "split_A=0 \n",
    "split_B=1 \n",
    "M = 10000\n",
    "N = 10000\n",
    "K = 10000\n",
    "A = ht.random.randn(M, N, split=split_A, device=\"gpu\")\n",
    "B = ht.random.randn(N, K, split=split_B, device=\"gpu\")\n",
    "C = ht.matmul(A, B)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b5829",
   "metadata": {},
   "source": [
    "## QR Decomposition and Triangular Solve\n",
    "\n",
    "Given a matrix $A$, its QR decomposition is given by $A=QR$ where $Q$ is an orthogonal matrix (i.e. its columns are pairwise orthonormal) and $R$ is an upper triangular matrix. \n",
    "\n",
    "Further information: [QR on Wikipedia](https://en.wikipedia.org/wiki/QR_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900afb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "A = ht.random.randn(100000, 1000, split=0, device=\"gpu\")\n",
    "Q,R = ht.linalg.qr(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43105d09",
   "metadata": {},
   "source": [
    "With a little bit of linear algebra fun, you find out that a linear least squares problem of type $\\min \\lVert Ax - b \\rVert_2$ boils down to computing the QR decomposition $A=QR$ and then solving for $Rx = Q^T b$. (Of course, we need to assume that if $A \\in \\mathbb{R}^{m \\times n}$ that $m \\geq n$ and $R$ is invertible...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "b = ht.random.randn(100000,split=None, device=\"gpu\")\n",
    "Qtb = Q.T @ b\n",
    "x = ht.linalg.solve_triangular(R,Qtb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7430496f",
   "metadata": {},
   "source": [
    "If you want to solve a LASSO-regularized version of this linear regression problem, try out `heat.regression.Lasso`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7e70bf",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "Given a matrix $X$, its singular value decomposition is defined to be $X = U \\Sigma V^T$ with orthogonal matrices $U, V$ and a diagonal matrix $\\Sigma$ with positive entries (the \"singular values\" of $X$). Further information: [SVD on Wikipedia](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "\n",
    "Computing the **full** SVD in a distributed environment can be quite **expensive**; nevertheless, we have implemented it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef68bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "A = ht.random.rand(2000,2000, split=1, device=\"gpu\")\n",
    "U, S, V = ht.linalg.svd(A)\n",
    "U, S, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec30705f",
   "metadata": {},
   "source": [
    "If the number of rows is much higher than the number of columns (we call such matrices \"tall-skinny\"), a more efficient implementation of SVD is available than in the general case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31438a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px \n",
    "X = ht.random.rand(100000,2000, split=0, device=\"gpu\")\n",
    "U, S, V = ht.linalg.svd(X) \n",
    "U, S, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb8960",
   "metadata": {},
   "source": [
    "Nevertheless, if you dont have a tall-skinny matrix, but only require an approximation of the largest singular values (and vectors)---and in many situations this should suffice---you can use, e.g., randomized SVD instead. In the following we use randomized SVD with a certain number of oversamples and one power iteration in order to compute an approximation to the leading 10 singular values and vectors of the same matrix $A$ as above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa5d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px \n",
    "Ur, Sr, Vr = ht.linalg.svd_randomized(X, 10, n_oversamples=10, power_iter=1)\n",
    "Ur, Sr, Vr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4fd4d4",
   "metadata": {},
   "source": [
    "## Exercises \n",
    "\n",
    "1. Try out different split combinations `split_A=0,1,None`, `split_B=0,1,None` for matrix-matrix multiplication. What do you observe for the split of the outcome? Does every combination take the same computing time? (Whats the likely cause for this?)\n",
    "\n",
    "2. Compare the approximate singular values computed by randomized SVD with the reference ones computed by full SVD. What do you observe for different choices of the number of oversamples and power iterations? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb8432",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
