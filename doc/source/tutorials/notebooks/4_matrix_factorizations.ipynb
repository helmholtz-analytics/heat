{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix factorizations\n",
    "\n",
    "### Refresher\n",
    "\n",
    "Using PyTorch as compute engine and mpi4py for communication, Heat implements a number of array operations and algorithms that are optimized for memory-distributed data volumes. This allows you to tackle datasets that are too large for single-node (or worse, single-GPU) processing. \n",
    "\n",
    "As opposed to task-parallel frameworks, Heat takes a data-parallel approach, meaning that each \"worker\" or MPI process performs the same tasks on different slices of the data. Many operations and algorithms are not embarassingly parallel, and involve data exchange between processes. Heat operations and algorithms are designed to minimize this communication overhead, and to make it transparent to the user.\n",
    "\n",
    "In other words: \n",
    "- you don't have to worry about optimizing data chunk sizes; \n",
    "- you don't have to make sure your research problem is embarassingly parallel, or artificially make your dataset smaller so your RAM is sufficient; \n",
    "- you do have to make sure that you have sufficient **overall** RAM to run your global task (e.g. number of nodes / GPUs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will demonstrate the usage of Heat's truncated SVD algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD and its truncated counterparts in a nutshell \n",
    "\n",
    "Let $X \\in \\mathbb{R}^{m \\times n}$ be a matrix, e.g., given by a data set consisting of $m$ data points $\\in \\mathbb{R}^n$ stacked together. The so-called **singular value decomposition (SVD)** of $X$ is given by \n",
    "\n",
    "$$\n",
    "\tX = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where $U \\in \\mathbb{R}^{m \\times r_X}$ and $V \\in \\mathbb{R}^{n \\times r_X}$ have orthonormal columns, $\\Sigma = \\text{diag}(\\sigma_1,...,\\sigma_{r_X}) \\in \\mathbb{R}^{r_X \\times r_X}$ is a diagonal matrix containing the so-called singular values $\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_{r_X} > 0$, and $r_X \\leq \\min(m,n)$ denotes the rank of $X$ (i.e. the dimension of the subspace of $\\mathbb{R}^m$ spanned by the columns of $X$). Since $\\Sigma = U^T X V$ is diagonal, one can imagine this decomposition as finding orthogonal coordinate transformations under which $X$ looks \"linear\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD in data science\n",
    "\n",
    "In data science, SVD is more often known as **principle component analysis (PCA)**, the columns of $U$ being called the principle components of $X$. In fact, in many applications **truncated SVD/PCA** suffices: to reduce $X$ to the \"essential\" information, one chooses a truncation rank $0 < r \\leq r_X$ and considers the truncated SVD/PCA given by \n",
    "\n",
    "$$\n",
    "X \\approx X_r := U_{[:,:r]} \\Sigma_{[:r,:r]} V_{[:,:r]}^T\n",
    "$$\n",
    "\n",
    "where we have used `numpy`-like notation for selecting only the first $r$ columns of $U$ and $V$, respectively. The rationale behind this is that if the first $r$ singular values of $X$ are much larger than the remaining ones, $X_r$ will still contain all \"essential\" information contained in $X$; in mathematical terms: \n",
    "\n",
    "$$\n",
    "\\lVert X_r - X \\rVert_{F}^2 = \\sum_{i=r+1}^{r_X} \\sigma_i^2, \n",
    "$$\n",
    "\n",
    "where $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm. Thus, truncated SVD/PCA may be used for, e.g.,  \n",
    "* filtering away non-essential information in order to get a \"feeling\" for the main characteristics of your data set, \n",
    "* to detect linear (or \"almost\" linear) dependencies in your data, \n",
    "* to generate features for further processing of your data. \n",
    "\n",
    "Moreover, there is a plenty of more advanced data analytics and data-based simulation techniques, such as, e.g., Proper Orthogonal Decomposition (POD) or Dynamic Mode Decomposition (DMD), that are based on SVD/PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated SVD in Heat\n",
    "\n",
    "In Heat we have currently implemented an algorithm for computing an approximate truncated SVD, where truncation takes place either w.r.t. a fixed truncation-rank (`heat.linalg.hsvd_rank`) or w.r.t. a desired accuracy (`heat.linalg.hsvd_rtol`). In the latter case it can be ensured that it holds for the \"reconstruction error\": \n",
    "\n",
    "$$\n",
    "\\frac{\\lVert X - U U^T X \\rVert_F}{\\lVert X \\rVert_F} \\overset{!}{\\leq} \\text{rtol},\n",
    "$$\n",
    "\n",
    "where $U$ denotes the approximate left-singular vectors of $X$ computed by `heat.linalg.hsvd_rtol`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the usage of Heat's truncated SVD algorithm, we will load the data set from the last example and then compute its truncated SVD. As usual, first we need to gain access to the MPI environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 engines found\n"
     ]
    }
   ],
   "source": [
    "from ipyparallel import Client\n",
    "rc = Client(profile=\"mpi\")\n",
    "rc.ids\n",
    "\n",
    "if len(rc.ids) == 0:\n",
    "    print(\"No engines found\")\n",
    "else:\n",
    "    print(f\"{len(rc.ids)} engines found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1:execute]\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mheat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mht\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X = \u001b[43mht\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_hdf5\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/p/scratch/training2404/data/JPL_SBDB/sbdb_asteroids.h5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m.T\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/heat_nb_env/lib64/python3.11/site-packages/heat/core/io.py:563\u001b[39m, in \u001b[36mload_hdf5\u001b[39m\u001b[34m(path, dataset, dtype, load_fraction, split, device, comm)\u001b[39m\n",
      "\u001b[32m    560\u001b[39m comm = sanitize_comm(comm)\n",
      "\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# actually load the data from the HDF5 file\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n",
      "\u001b[32m    564\u001b[39m     data = handle[dataset]\n",
      "\u001b[32m    565\u001b[39m     gshape = data.shape\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/heat_nb_env/lib64/python3.11/site-packages/h5py/_hl/files.py:564\u001b[39m, in \u001b[36mFile.__init__\u001b[39m\u001b[34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[39m\n",
      "\u001b[32m    555\u001b[39m     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n",
      "\u001b[32m    556\u001b[39m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n",
      "\u001b[32m    557\u001b[39m                      alignment_threshold=alignment_threshold,\n",
      "\u001b[32m    558\u001b[39m                      alignment_interval=alignment_interval,\n",
      "\u001b[32m    559\u001b[39m                      meta_block_size=meta_block_size,\n",
      "\u001b[32m    560\u001b[39m                      **kwds)\n",
      "\u001b[32m    561\u001b[39m     fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n",
      "\u001b[32m    562\u001b[39m                      fs_persist=fs_persist, fs_threshold=fs_threshold,\n",
      "\u001b[32m    563\u001b[39m                      fs_page_size=fs_page_size)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     fid = \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[32m    567\u001b[39m     \u001b[38;5;28mself\u001b[39m._libver = libver\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/heat_nb_env/lib64/python3.11/site-packages/h5py/_hl/files.py:238\u001b[39m, in \u001b[36mmake_fid\u001b[39m\u001b[34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[39m\n",
      "\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n",
      "\u001b[32m    237\u001b[39m         flags |= h5f.ACC_SWMR_READ\n",
      "\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     fid = \u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    239\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[32m    240\u001b[39m     fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:54\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:55\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5f.pyx:102\u001b[39m, in \u001b[36mh5py.h5f.open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/p/scratch/training2404/data/JPL_SBDB/sbdb_asteroids.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "[0:execute]\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mheat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mht\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X = \u001b[43mht\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_hdf5\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/p/scratch/training2404/data/JPL_SBDB/sbdb_asteroids.h5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m.T\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/heat_nb_env/lib64/python3.11/site-packages/heat/core/io.py:563\u001b[39m, in \u001b[36mload_hdf5\u001b[39m\u001b[34m(path, dataset, dtype, load_fraction, split, device, comm)\u001b[39m\n",
      "\u001b[32m    560\u001b[39m comm = sanitize_comm(comm)\n",
      "\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# actually load the data from the HDF5 file\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n",
      "\u001b[32m    564\u001b[39m     data = handle[dataset]\n",
      "\u001b[32m    565\u001b[39m     gshape = data.shape\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/heat_nb_env/lib64/python3.11/site-packages/h5py/_hl/files.py:564\u001b[39m, in \u001b[36mFile.__init__\u001b[39m\u001b[34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[39m\n",
      "\u001b[32m    555\u001b[39m     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n",
      "\u001b[32m    556\u001b[39m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n",
      "\u001b[32m    557\u001b[39m                      alignment_threshold=alignment_threshold,\n",
      "\u001b[32m    558\u001b[39m                      alignment_interval=alignment_interval,\n",
      "\u001b[32m    559\u001b[39m                      meta_block_size=meta_block_size,\n",
      "\u001b[32m    560\u001b[39m                      **kwds)\n",
      "\u001b[32m    561\u001b[39m     fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n",
      "\u001b[32m    562\u001b[39m                      fs_persist=fs_persist, fs_threshold=fs_threshold,\n",
      "\u001b[32m    563\u001b[39m                      fs_page_size=fs_page_size)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     fid = \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[32m    567\u001b[39m     \u001b[38;5;28mself\u001b[39m._libver = libver\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/heat_nb_env/lib64/python3.11/site-packages/h5py/_hl/files.py:238\u001b[39m, in \u001b[36mmake_fid\u001b[39m\u001b[34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[39m\n",
      "\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n",
      "\u001b[32m    237\u001b[39m         flags |= h5f.ACC_SWMR_READ\n",
      "\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     fid = \u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    239\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[32m    240\u001b[39m     fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:54\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:55\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5f.pyx:102\u001b[39m, in \u001b[36mh5py.h5f.open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/p/scratch/training2404/data/JPL_SBDB/sbdb_asteroids.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "[2:execute]\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mheat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mht\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X = \u001b[43mht\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_hdf5\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/p/scratch/training2404/data/JPL_SBDB/sbdb_asteroids.h5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m.T\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/heat_nb_env/lib64/python3.11/site-packages/heat/core/io.py:563\u001b[39m, in \u001b[36mload_hdf5\u001b[39m\u001b[34m(path, dataset, dtype, load_fraction, split, device, comm)\u001b[39m\n",
      "\u001b[32m    560\u001b[39m comm = sanitize_comm(comm)\n",
      "\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# actually load the data from the HDF5 file\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n",
      "\u001b[32m    564\u001b[39m     data = handle[dataset]\n",
      "\u001b[32m    565\u001b[39m     gshape = data.shape\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/heat_nb_env/lib64/python3.11/site-packages/h5py/_hl/files.py:564\u001b[39m, in \u001b[36mFile.__init__\u001b[39m\u001b[34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[39m\n",
      "\u001b[32m    555\u001b[39m     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n",
      "\u001b[32m    556\u001b[39m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n",
      "\u001b[32m    557\u001b[39m                      alignment_threshold=alignment_threshold,\n",
      "\u001b[32m    558\u001b[39m                      alignment_interval=alignment_interval,\n",
      "\u001b[32m    559\u001b[39m                      meta_block_size=meta_block_size,\n",
      "\u001b[32m    560\u001b[39m                      **kwds)\n",
      "\u001b[32m    561\u001b[39m     fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n",
      "\u001b[32m    562\u001b[39m                      fs_persist=fs_persist, fs_threshold=fs_threshold,\n",
      "\u001b[32m    563\u001b[39m                      fs_page_size=fs_page_size)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     fid = \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[32m    567\u001b[39m     \u001b[38;5;28mself\u001b[39m._libver = libver\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/heat_nb_env/lib64/python3.11/site-packages/h5py/_hl/files.py:238\u001b[39m, in \u001b[36mmake_fid\u001b[39m\u001b[34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[39m\n",
      "\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n",
      "\u001b[32m    237\u001b[39m         flags |= h5f.ACC_SWMR_READ\n",
      "\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     fid = \u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    239\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[32m    240\u001b[39m     fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:54\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:55\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5f.pyx:102\u001b[39m, in \u001b[36mh5py.h5f.open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/p/scratch/training2404/data/JPL_SBDB/sbdb_asteroids.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "[3:execute]\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mheat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mht\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X = \u001b[43mht\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_hdf5\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/p/scratch/training2404/data/JPL_SBDB/sbdb_asteroids.h5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m.T\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/heat_nb_env/lib64/python3.11/site-packages/heat/core/io.py:563\u001b[39m, in \u001b[36mload_hdf5\u001b[39m\u001b[34m(path, dataset, dtype, load_fraction, split, device, comm)\u001b[39m\n",
      "\u001b[32m    560\u001b[39m comm = sanitize_comm(comm)\n",
      "\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# actually load the data from the HDF5 file\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n",
      "\u001b[32m    564\u001b[39m     data = handle[dataset]\n",
      "\u001b[32m    565\u001b[39m     gshape = data.shape\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/heat_nb_env/lib64/python3.11/site-packages/h5py/_hl/files.py:564\u001b[39m, in \u001b[36mFile.__init__\u001b[39m\u001b[34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[39m\n",
      "\u001b[32m    555\u001b[39m     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n",
      "\u001b[32m    556\u001b[39m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n",
      "\u001b[32m    557\u001b[39m                      alignment_threshold=alignment_threshold,\n",
      "\u001b[32m    558\u001b[39m                      alignment_interval=alignment_interval,\n",
      "\u001b[32m    559\u001b[39m                      meta_block_size=meta_block_size,\n",
      "\u001b[32m    560\u001b[39m                      **kwds)\n",
      "\u001b[32m    561\u001b[39m     fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n",
      "\u001b[32m    562\u001b[39m                      fs_persist=fs_persist, fs_threshold=fs_threshold,\n",
      "\u001b[32m    563\u001b[39m                      fs_page_size=fs_page_size)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     fid = \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[32m    567\u001b[39m     \u001b[38;5;28mself\u001b[39m._libver = libver\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/heat_nb_env/lib64/python3.11/site-packages/h5py/_hl/files.py:238\u001b[39m, in \u001b[36mmake_fid\u001b[39m\u001b[34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[39m\n",
      "\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n",
      "\u001b[32m    237\u001b[39m         flags |= h5f.ACC_SWMR_READ\n",
      "\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     fid = \u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    239\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[32m    240\u001b[39m     fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:54\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:55\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5f.pyx:102\u001b[39m, in \u001b[36mh5py.h5f.open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/p/scratch/training2404/data/JPL_SBDB/sbdb_asteroids.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    },
    {
     "ename": "AlreadyDisplayedError",
     "evalue": "4 errors",
     "output_type": "error",
     "traceback": [
      "4 errors"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import heat as ht\n",
    "X = ht.load_hdf5(\"/p/scratch/training2404/data/JPL_SBDB/sbdb_asteroids.h5\",dataset=\"data\",split=0).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that due to the transpose, `X` is distributed along the columns now; this is required by the hSVD-algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first compute the truncated SVD by setting the relative tolerance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# compute truncated SVD w.r.t. relative tolerance \n",
    "svd_with_reltol = ht.linalg.hsvd_rtol(X,rtol=1.0e-2,compute_sv=True,silent=False)\n",
    "print(\"relative residual:\", svd_with_reltol[3], \"rank: \", svd_with_reltol[0].shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can compute a truncated SVD with a fixed truncation rank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# compute truncated SVD w.r.t. a fixed truncation rank \n",
    "svd_with_rank = ht.linalg.hsvd_rank(X, maxrank=3,compute_sv=True,silent=False)\n",
    "print(\"relative residual:\", svd_with_rank[3], \"rank: \", svd_with_rank[0].shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have computed the truncated SVD, we can use it to approximate the original data matrix `X` by the truncated matrix `X_r`. \n",
    "\n",
    "Check out the plot below to see how Heat's truncated SVD algorithm scales with the number of MPI processes and size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=https://github.com/helmholtz-analytics/heat/blob/main/doc/images/hSVD_bench_rank5.png?raw=true title=\"hSVD rank 5\" width=\"30%\" style=\"float:left\"/>\n",
    "  <img src=https://github.com/helmholtz-analytics/heat/blob/main/doc/images/hSVD_bench_rank50.png?raw=true title=\"hSVD rank 50\" width=\"30%\" style=\"float:center \"/>\n",
    "  <img src=https://github.com/helmholtz-analytics/heat/blob/main/doc/images/hSVD_bench_rank500.png?raw=true title=\"HSVD rank 500\" width=\"30%\" style=\"float:center\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other factorizations\n",
    "\n",
    "Other common factorization algorithms are supported in Heat, such as:\n",
    "- QR decomposition (`heat.linalg.qr`),\n",
    "- Lanczos algorithm for computing the largest eigenvalues and corresponding eigenvectors (`heat.linalg.lanczos`)\n",
    "\n",
    "Check out our [`linalg` PRs](https://github.com/helmholtz-analytics/heat/pulls?q=is%3Aopen+is%3Apr+label%3Alinalg) to see what's in progress.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References for hierarchical SVD**\n",
    "\n",
    "1. Iwen, Ong. *A distributed and incremental SVD algorithm for agglomerative data analysis on large networks.* SIAM J. Matrix Anal. Appl., **37** (4), 2016.\n",
    "2. Himpe, Leibner, Rave. *Hierarchical approximate proper orthogonal decomposition.* SIAM J. Sci. Comput., **4** (5), 2018.\n",
    "3. Halko, Martinsson, Tropp. *Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions.* SIAM Rev. 53, **2** (2011)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (heat_nb_env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
